{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxillary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(path, blur_radius = 6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (string): Path to the folder containing the images to be filtered.\n",
    "        blur_radius (int, optional): Blur radius for the median filter. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        None: All information gets saved in the path + 'median' folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read images\n",
    "    accepted_filetypes = ['tif', 'tiff', 'png', 'jpg', 'jpeg', 'bmp']\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(path+'median')\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1][1:] # Get file extension\n",
    "        if ext.lower() not in accepted_filetypes:\n",
    "            continue\n",
    "        cv.imwrite(path+'median/'+f+'.tiff' ,(cv.imread(path+f, cv.IMREAD_GRAYSCALE).astype(np.float32)-cv.medianBlur(cv.imread(path+f, cv.IMREAD_GRAYSCALE).astype(np.float32),blur_radius)))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images to PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_pt(path):\n",
    "    '''Converts all images in a folder to a pyTorch tensor.\n",
    "    Args:\n",
    "        path (string): Path to the folder containing the images.\n",
    "        \n",
    "    Returns:\n",
    "        tensor_imgs (torch.Tensor): PyTorch Tensor containing all the images.\n",
    "    '''\n",
    "    # Find all images in a folder:\n",
    "    imgs = []\n",
    "    accepted_filetypes = ['tif', 'tiff', 'png', 'jpg', 'jpeg', 'bmp']\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1][1:]\n",
    "        if ext.lower() not in accepted_filetypes:\n",
    "            continue\n",
    "        imgs.append(cv.imread(path+f, cv.IMREAD_UNCHANGED).astype(np.float32))\n",
    "\n",
    "    # Convert them into a tensor for pyTorch\n",
    "    tensor_imgs = torch.from_numpy(np.array(imgs,dtype=np.float32)).to(torch.float32)\n",
    "    del imgs\n",
    "    gc.collect()\n",
    "    \n",
    "    return tensor_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .pt to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_to_img(path):\n",
    "    # Find all images in a folder:\n",
    "    imgs = []\n",
    "    accepted_filetypes = ['pt']\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1][1:]\n",
    "        if ext.lower() not in accepted_filetypes:\n",
    "            continue\n",
    "        for i in range(len(torch.load(path+f, map_location=torch.device('cpu')))):\n",
    "            cv.imwrite(path+f+'_'+str(i)+'.tif', torch.load(path+f, map_location=torch.device('cpu'))[i,:,:].numpy())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_to_img(r'./charles_data/flat_back/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_to_img(r'./charles_data/batch1/set14_img156/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_to_img(r'./charles_data/batch1/set14_img133/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_to_img(r'./charles_data/batch1/set13_img169/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $2^n$ for FFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_n_squareify(tensor):\n",
    "    '''\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor to be have its dimensions rounded up to the nearest 2^n. Should be in the format of [depth, x, y].\n",
    "    Returns:\n",
    "        return_tensor (torch.Tensor): Tensor with its dimensions rounded up to the nearest 2^n. Image will first be croped to 2^n-1/2^n-1, then padded with zeros to 2^n/2^n. This is to ensure linear convolution in the Fourier domain.\n",
    "    '''\n",
    "    # This function takes a tensor and pads it with zeros until the dimensions are a power of 2\n",
    "    # The return tensor is a square tensor\n",
    "    \n",
    "    # Find the next power of 2\n",
    "    next_power = max(2**math.ceil(math.log2(len(tensor[0,:,0]))), 2**math.ceil(math.log2(len(tensor[0,0,:]))))\n",
    "    \n",
    "    # Find the amount of padding needed\n",
    "    x_pad = next_power - len(tensor[0,:,0])\n",
    "    y_pad = next_power - len(tensor[0,0,:])\n",
    "    \n",
    "    # Check to see if it's even or odd\n",
    "    if x_pad % 2 == 0:\n",
    "        l_x_pad = int(x_pad/2)\n",
    "        r_x_pad = int(x_pad/2)\n",
    "    else:\n",
    "        l_x_pad = int(x_pad/2)\n",
    "        r_x_pad = x_pad - int(x_pad/2)\n",
    "    \n",
    "    if y_pad % 2 == 0:\n",
    "        l_y_pad = int(y_pad/2)\n",
    "        r_y_pad = int(y_pad/2)\n",
    "    else:\n",
    "        l_y_pad = int(y_pad/2)\n",
    "        r_y_pad = y_pad - int(y_pad/2)\n",
    "    \n",
    "    mask_tensor = torch.ones(len(tensor[:,0,0]), next_power, next_power)\n",
    "    \n",
    "    mask_tensor[:,0:int(next_power/4),:] = 0\n",
    "    mask_tensor[:,:,0:int(next_power/4)] = 0\n",
    "    mask_tensor[:,int(3*next_power/4):,:] = 0\n",
    "    mask_tensor[:,:,int(3*next_power/4):] = 0\n",
    "    \n",
    "    return_tensor = torch.mul(torch.nn.functional.pad(tensor, (l_y_pad, r_y_pad, l_x_pad, r_x_pad), \"constant\", 0), mask_tensor)\n",
    "    \n",
    "    return return_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(two_n_squareify(torch.from_numpy(cv.imread(r'./charles_data/deformed_reference.tif', cv.IMREAD_UNCHANGED).astype(np.float32)).unsqueeze(0)).to(torch.float32), r'./charles_data/deformed_2n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(two_n_squareify(torch.from_numpy(cv.imread(r'./charles_data/batch1/set13_img169.tif', cv.IMREAD_UNCHANGED).astype(np.float32)).unsqueeze(0)).to(torch.float32), r'./charles_data/batch1/set13_img169.pt')\n",
    "torch.save(two_n_squareify(torch.from_numpy(cv.imread(r'./charles_data/batch1/set14_img133.tif', cv.IMREAD_UNCHANGED).astype(np.float32)).unsqueeze(0)).to(torch.float32), r'./charles_data/batch1/set14_img133.pt')\n",
    "torch.save(two_n_squareify(torch.from_numpy(cv.imread(r'./charles_data/batch1/set14_img156.tif', cv.IMREAD_UNCHANGED).astype(np.float32)).unsqueeze(0)).to(torch.float32), r'./charles_data/batch1/set14_img156.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(two_n_squareify(torch.from_numpy(cv.imread(r'./charles_data/deformed_1.tif', cv.IMREAD_UNCHANGED)).unsqueeze(0)).to(torch.float32), r'./charles_data/deformed_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(two_n_squareify(torch.load(r'./charles_data/Ht_revision.pt')), r'./charles_data/Ht_rev_2n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_psf(path, normalization_type = None):\n",
    "    # Loads the PSF stack from a path\n",
    "    #path = r'/Users/halensolomon/Code/FLFM_local/testing/psf_stack.pt'\n",
    "    \n",
    "    # if the file type is .pt, then we can just load it\n",
    "    if path[-3:] == '.pt':\n",
    "        psf = torch.load(path)\n",
    "        if normalization_type == 'forward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (0))) # Sum of each axial slice should be 1\n",
    "        if normalization_type == 'backward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (1,2))) # Sum of each axial slice should be 1\n",
    "    \n",
    "    # if the path is actually a folder, then we need to load in all the files in the folder\n",
    "    elif os.path.isdir(path):\n",
    "        psf = torch.zeros(len(os.listdir(path)), len(Image.open(path + '/' + os.listdir(path)[0]).convert('L').getdata()), len(Image.open(path + '/' + os.listdir(path)[0]).convert('L').getdata()))\n",
    "        for i in range(len(os.listdir(path))): # Format is [depth, x, y]\n",
    "            psf[i,:,:] = torch.from_numpy(np.array(Image.open(path + '/' + os.listdir(path)[i]).convert('L').getdata()).reshape(Image.open(path + '/' + os.listdir(path)[i]).convert('L').size))\n",
    "        if normalization_type == 'forward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (0)))\n",
    "        if normalization_type == 'backward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (1,2)))\n",
    "        \n",
    "    return psf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut PSF to FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fov_cut(PSF, fov_radius):\n",
    "    def circ(x, y, r):\n",
    "        return (x**2 + y**2 < r**2).to(int)\n",
    "    \n",
    "    x = torch.range(0, len(PSF[0,0,:])-1)\n",
    "    y = torch.range(0, len(PSF[0,:,0])-1)\n",
    "    \n",
    "    PSF_return = torch.empty(len(PSF[:,0,0]), len(PSF[0,0,:]), len(PSF[0,:,0]))\n",
    "    \n",
    "    for i in range(len(PSF[:,0,0])):\n",
    "        PSF_return[i,:,:] = torch.mul(PSF[i,:,:], circ(x, y, fov_radius))\n",
    "    \n",
    "    return PSF_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSF normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_b_norm(forward, backward, path):\n",
    "    '''\n",
    "    Forward: [depth, x, y], where depth is the same as the backward model\n",
    "    Backward: [depth, x, y], where depth is the same as the forward model\n",
    "    Path: The path to save the normalized forward and backward models (ex. r'/Users/yourname/yourfolder/)\n",
    "    '''\n",
    "    \n",
    "    # Normalize so that the each forward depth slice sums to 1\n",
    "    forward[forward <= 1e-8] = 1e-8 # Computational trick to avoid dividing by zero\n",
    "    forward[:,:,:] = torch.div(forward[:,:,:], torch.sum(forward, dim = (1,2)).unsqueeze(1).unsqueeze(2)) # Brodcast the sum of the forward model to the same shape as the forward model\n",
    "    forward[forward == 1/len(forward[:,0,0])] = 0 # Set the values that had no light to zero\n",
    "    \n",
    "    # Normalize so that each voxel for the backward model sums to 1\n",
    "    backward[backward <= 1e-8] = 1e-8 # Computational trick to avoid dividing by zero\n",
    "    backward[:,:,:] = torch.div(backward[:,:,:], torch.sum(backward, dim = (0)).unsqueeze(0)) # Brodcast the sum of the backward model to the same shape as the backward model\n",
    "    backward[backward == 1/len(backward[:,0,0])] = 0 # Set the values that had no light to zero\n",
    "    \n",
    "    # Save the normalized forward and backward models\n",
    "    torch.save(forward, path + \"forward_norm.pt\")\n",
    "    torch.save(backward, path + \"backward_norm.pt\")\n",
    "    \n",
    "    return None # This function is only used to save the normalized forward and backward models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .mat to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_to_torch(file):\n",
    "    '''\n",
    "    Args:\n",
    "        file (string): Path to the .mat file to be converted to a torch tensor.\n",
    "    Returns:\n",
    "        torch_tensor (torch.Tensor): PyTorch tensor containing the data from the .mat file.\n",
    "    '''\n",
    "    sparse_matrix = sio.loadmat(file, struct_as_record=False)\n",
    "    sparse_matrix = sparse_matrix[list(sparse_matrix.keys())[-1]]\n",
    "    torch_tensor = torch.empty(len(sparse_matrix[0,0,:]), sparse_matrix[0,0,0].shape[0], sparse_matrix[0,0,0].shape[1])\n",
    "    \n",
    "    for i in range(len(sparse_matrix[0,0,:])):\n",
    "        temp_matrix = sparse_matrix[0,0,i].toarray()\n",
    "        torch_tensor[i, :, :] = torch.from_numpy(temp_matrix)\n",
    "    \n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Rescaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_rescaler(size, tensor):\n",
    "    # Assuming the tensor is in the format of [depth, x, y]\n",
    "    rescaled_tensor = tv.transforms.Resize(size = size)(tensor[:,:,:])\n",
    "    return rescaled_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .pt to .tif"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
