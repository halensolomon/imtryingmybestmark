{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision as tv\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxillary Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Median Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(path, blur_radius = 6):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path (string): Path to the folder containing the images to be filtered.\n",
    "        blur_radius (int, optional): Blur radius for the median filter. Defaults to 6.\n",
    "\n",
    "    Returns:\n",
    "        None: All information gets saved in the path + 'median' folder.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read images\n",
    "    accepted_filetypes = ['tif', 'tiff', 'png', 'jpg', 'jpeg', 'bmp']\n",
    "    \n",
    "    try:\n",
    "        os.mkdir(path+'median')\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1][1:] # Get file extension\n",
    "        if ext.lower() not in accepted_filetypes:\n",
    "            continue\n",
    "        cv.imwrite(path+'median/'+f+'.tiff' ,(cv.imread(path+f, cv.IMREAD_GRAYSCALE).astype(np.float32)-cv.medianBlur(cv.imread(path+f, cv.IMREAD_GRAYSCALE).astype(np.float32),blur_radius)))\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images to PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_pt(path):\n",
    "    '''Converts all images in a folder to a pyTorch tensor.\n",
    "    Args:\n",
    "        path (string): Path to the folder containing the images.\n",
    "        \n",
    "    Returns:\n",
    "        tensor_imgs (torch.Tensor): PyTorch Tensor containing all the images.\n",
    "    '''\n",
    "    # Find all images in a folder:\n",
    "    imgs = []\n",
    "    accepted_filetypes = ['tif', 'tiff', 'png', 'jpg', 'jpeg', 'bmp']\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1][1:]\n",
    "        if ext.lower() not in accepted_filetypes:\n",
    "            continue\n",
    "        imgs.append(cv.imread(path+f, cv.IMREAD_UNCHANGED).astype(np.float32))\n",
    "\n",
    "    # Convert them into a tensor for pyTorch\n",
    "    tensor_imgs = torch.from_numpy(np.array(imgs,dtype=np.float32)).to(torch.float32)\n",
    "    del imgs\n",
    "    gc.collect()\n",
    "    \n",
    "    return tensor_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .pt to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt_to_img(path):\n",
    "    # Find all images in a folder:\n",
    "    imgs = []\n",
    "    accepted_filetypes = ['pt']\n",
    "    for f in os.listdir(path):\n",
    "        ext = os.path.splitext(f)[1][1:]\n",
    "        if ext.lower() not in accepted_filetypes:\n",
    "            continue\n",
    "        for i in range(len(torch.load(path+f, map_location=torch.device('cpu')))):\n",
    "            cv.imwrite(path+f+'_'+str(i)+'.tif', torch.load(path+f, map_location=torch.device('cpu'))[i,:,:].numpy())\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './charles_data/flat_back/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pt_to_img(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./charles_data/flat_back/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m, in \u001b[0;36mpt_to_img\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      3\u001b[0m imgs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m accepted_filetypes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(path):\n\u001b[0;32m      6\u001b[0m     ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(f)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m accepted_filetypes:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './charles_data/flat_back/'"
     ]
    }
   ],
   "source": [
    "pt_to_img(r'./charles_data/flat_back/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_to_img(r'./charles_data/batch1/set14_img156/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_to_img(r'./charles_data/batch1/set14_img133/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_to_img(r'./charles_data/batch1/set13_img169/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $2^n$ for FFTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_n_squareify(tensor):\n",
    "    '''\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Input tensor to be have its dimensions rounded up to the nearest 2^n. Should be in the format of [depth, x, y].\n",
    "    Returns:\n",
    "        return_tensor (torch.Tensor): Tensor with its dimensions rounded up to the nearest 2^n. Image will first be croped to 2^n-1/2^n-1, then padded with zeros to 2^n/2^n. This is to ensure linear convolution in the Fourier domain.\n",
    "    '''\n",
    "    # This function takes a tensor and pads it with zeros until the dimensions are a power of 2\n",
    "    # The return tensor is a square tensor\n",
    "\n",
    "    old_shape = tensor.shape\n",
    "\n",
    "    #check and make sure .pt is in format of (depth, x, y)\n",
    "    if len(tensor.shape) == 3:\n",
    "        if tensor.shape[-1] > tensor.shape[0]:\n",
    "            pass\n",
    "        else:\n",
    "            # this means .pt is in format of (x, y, depth)\n",
    "            tensor = tensor.permute(2,0,1)\n",
    "            print('Transposed tensor to (depth, x, y) format', str(tensor.shape))\n",
    "        \n",
    "    \n",
    "    # Find the next power of 2\n",
    "    next_power = max(2**math.ceil(math.log2(len(tensor[0,:,0]))), 2**math.ceil(math.log2(len(tensor[0,0,:]))))\n",
    "\n",
    "    # Check if the tensor is already a power of 2\n",
    "    if np.log2(len(tensor[0,:,0])).is_integer:\n",
    "        next_power = int(2 * len(tensor[0,:,0]))\n",
    "        pad = int(next_power / 4)\n",
    "        return_tensor = torch.nn.functional.pad(tensor, (pad, pad, pad, pad), \"constant\", 0)\n",
    "\n",
    "    else: \n",
    "    \n",
    "        # Find the amount of padding needed\n",
    "        x_pad = next_power - len(tensor[0,:,0])\n",
    "        y_pad = next_power - len(tensor[0,0,:])\n",
    "\n",
    "        # Check to see if it's even or odd\n",
    "        if x_pad % 2 == 0:\n",
    "            l_x_pad = int(x_pad/2)\n",
    "            r_x_pad = int(x_pad/2)\n",
    "        else:\n",
    "            l_x_pad = int(x_pad/2)\n",
    "            r_x_pad = x_pad - int(x_pad/2)\n",
    "\n",
    "        if y_pad % 2 == 0:\n",
    "            l_y_pad = int(y_pad/2)\n",
    "            r_y_pad = int(y_pad/2)\n",
    "        else:\n",
    "            l_y_pad = int(y_pad/2)\n",
    "            r_y_pad = y_pad - int(y_pad/2)\n",
    "\n",
    "        mask_tensor = torch.ones(len(tensor[:,0,0]), next_power, next_power)\n",
    "\n",
    "        mask_tensor[:,0:int(next_power/4),:] = 0\n",
    "        mask_tensor[:,:,0:int(next_power/4)] = 0\n",
    "        mask_tensor[:,int(3*next_power/4):,:] = 0\n",
    "        mask_tensor[:,:,int(3*next_power/4):] = 0\n",
    "\n",
    "        return_tensor = torch.mul(torch.nn.functional.pad(tensor, (l_y_pad, r_y_pad, l_x_pad, r_x_pad), \"constant\", 0), mask_tensor)\n",
    "    \n",
    "    new_shape = return_tensor.shape\n",
    "\n",
    "    print('Old shape: '+ str(old_shape) +', '+ ' New shape: ' + str(new_shape))\n",
    "\n",
    "    return return_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old shape: torch.Size([1, 2048, 2048]),  New shape: torch.Size([1, 4096, 4096])\n",
      "Transposed tensor to (depth, x, y) format torch.Size([100, 2048, 2048])\n",
      "Old shape: torch.Size([2048, 2048, 100]),  New shape: torch.Size([100, 4096, 4096])\n",
      "Transposed tensor to (depth, x, y) format torch.Size([100, 2048, 2048])\n",
      "Old shape: torch.Size([2048, 2048, 100]),  New shape: torch.Size([100, 4096, 4096])\n"
     ]
    }
   ],
   "source": [
    "torch.save(two_n_squareify(torch.load(r'./frames.pt')), r'./frames_2n.pt')\n",
    "torch.save(two_n_squareify(torch.load(r'./forward_test.pt')), r'./forward_test_2n.pt')\n",
    "torch.save(two_n_squareify(torch.load(r'./backward_test.pt')), r'./backward_test_2n.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_psf(path, normalization_type = None):\n",
    "    # Loads the PSF stack from a path\n",
    "    #path = r'/Users/halensolomon/Code/FLFM_local/testing/psf_stack.pt'\n",
    "    \n",
    "    # if the file type is .pt, then we can just load it\n",
    "    if path[-3:] == '.pt':\n",
    "        psf = torch.load(path)\n",
    "        if normalization_type == 'forward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (0))) # Sum of each axial slice should be 1\n",
    "        if normalization_type == 'backward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (1,2))) # Sum of each axial slice should be 1\n",
    "    \n",
    "    # if the path is actually a folder, then we need to load in all the files in the folder\n",
    "    elif os.path.isdir(path):\n",
    "        psf = torch.zeros(len(os.listdir(path)), len(Image.open(path + '/' + os.listdir(path)[0]).convert('L').getdata()), len(Image.open(path + '/' + os.listdir(path)[0]).convert('L').getdata()))\n",
    "        for i in range(len(os.listdir(path))): # Format is [depth, x, y]\n",
    "            psf[i,:,:] = torch.from_numpy(np.array(Image.open(path + '/' + os.listdir(path)[i]).convert('L').getdata()).reshape(Image.open(path + '/' + os.listdir(path)[i]).convert('L').size))\n",
    "        if normalization_type == 'forward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (0)))\n",
    "        if normalization_type == 'backward':\n",
    "            psf = torch.div(psf, torch.sum(psf, dim = (1,2)))\n",
    "        \n",
    "    return psf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut PSF to FOV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fov_cut(PSF, fov_radius):\n",
    "    def circ(x, y, r):\n",
    "        return (x**2 + y**2 < r**2).to(int)\n",
    "    \n",
    "    x = torch.range(0, len(PSF[0,0,:])-1)\n",
    "    y = torch.range(0, len(PSF[0,:,0])-1)\n",
    "    \n",
    "    PSF_return = torch.empty(len(PSF[:,0,0]), len(PSF[0,0,:]), len(PSF[0,:,0]))\n",
    "    \n",
    "    for i in range(len(PSF[:,0,0])):\n",
    "        PSF_return[i,:,:] = torch.mul(PSF[i,:,:], circ(x, y, fov_radius))\n",
    "    \n",
    "    return PSF_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSF normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_b_norm(forward, backward, path):\n",
    "    '''\n",
    "    Forward: [depth, x, y], where depth is the same as the backward model\n",
    "    Backward: [depth, x, y], where depth is the same as the forward model\n",
    "    Path: The path to save the normalized forward and backward models (ex. r'/Users/yourname/yourfolder/)\n",
    "    '''\n",
    "    \n",
    "    # Normalize so that the each forward depth slice sums to 1\n",
    "    forward[forward <= 1e-8] = 1e-8 # Computational trick to avoid dividing by zero\n",
    "    forward[:,:,:] = torch.div(forward[:,:,:], torch.sum(forward, dim = (1,2)).unsqueeze(1).unsqueeze(2)) # Brodcast the sum of the forward model to the same shape as the forward model\n",
    "    forward[forward == 1/len(forward[:,0,0])] = 0 # Set the values that had no light to zero\n",
    "    \n",
    "    # Normalize so that each voxel for the backward model sums to 1\n",
    "    backward[backward <= 1e-8] = 1e-8 # Computational trick to avoid dividing by zero\n",
    "    backward[:,:,:] = torch.div(backward[:,:,:], torch.sum(backward, dim = (0)).unsqueeze(0)) # Brodcast the sum of the backward model to the same shape as the backward model\n",
    "    backward[backward == 1/len(backward[:,0,0])] = 0 # Set the values that had no light to zero\n",
    "    \n",
    "    # Save the normalized forward and backward models\n",
    "    torch.save(forward, path + \"forward_norm.pt\")\n",
    "    torch.save(backward, path + \"backward_norm.pt\")\n",
    "    \n",
    "    return None # This function is only used to save the normalized forward and backward models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .mat to PyTorch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_to_torch(file):\n",
    "    '''\n",
    "    Args:\n",
    "        file (string): Path to the .mat file to be converted to a torch tensor.\n",
    "    Returns:\n",
    "        torch_tensor (torch.Tensor): PyTorch tensor containing the data from the .mat file.\n",
    "    '''\n",
    "    sparse_matrix = sio.loadmat(file, struct_as_record=False)\n",
    "    sparse_matrix = sparse_matrix[list(sparse_matrix.keys())[-1]]\n",
    "    torch_tensor = torch.empty(len(sparse_matrix[0,0,:]), sparse_matrix[0,0,0].shape[0], sparse_matrix[0,0,0].shape[1])\n",
    "    \n",
    "    for i in range(len(sparse_matrix[0,0,:])):\n",
    "        temp_matrix = sparse_matrix[0,0,i].toarray()\n",
    "        torch_tensor[i, :, :] = torch.from_numpy(temp_matrix)\n",
    "    \n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Rescaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_rescaler(size, tensor):\n",
    "    # Assuming the tensor is in the format of [depth, x, y]\n",
    "    rescaled_tensor = tv.transforms.Resize(size = size)(tensor[:,:,:])\n",
    "    return rescaled_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .pt to .tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2 as cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pt2tif(file, path):\n",
    "    tens = torch.load(file).to(device = 'cpu')\n",
    "    for i in range(len(tens[:,0,0])):\n",
    "        cv.imwrite(path + str(i) + '.tif', tens[i,:,:].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt2tif(r'./algae/0itr10.pt', r'./10itr/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "halen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
